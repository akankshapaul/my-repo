{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## Next word prediction involves developing a model capable of predicting the most probable word to follow a given sequence of words in a sentence\n",
        "\n",
        "## The task is to develop a robust Next Word Prediction model that can accurately forecast the most appropriate word following a given sequence of words.\n",
        "\n",
        "## By analyzing the dataset inspired by Sherlock Holmes, the model should learn the linguistic patterns and relationships that govern the progression of words."
      ],
      "metadata": {
        "id": "roerG2KFfcl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Importing necessary libraries and dataset\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "\n",
        "# Read the text file\n",
        "with open('/content/drive/MyDrive/Atomic_Habits_by_James_Clear.txt', 'r', encoding = 'utf-8') as file :\n",
        "  text = file.read()"
      ],
      "metadata": {
        "id": "OVwqDXFBgFY9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokenize text to create a sequence of words\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "WRzce1wcjAL1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Prepare the data by creating input-output pairs by splitting the text into sequences of tokens and forming n-grams from the sequences\n",
        "\n",
        "input_sequences = []\n",
        "for line in text.split('\\n'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)"
      ],
      "metadata": {
        "id": "uYhT9in7kXde"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Pad input sequences to have equal length\n",
        "\n",
        "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
      ],
      "metadata": {
        "id": "0eNrAhnrkkeP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Split sequences into input and output\n",
        "\n",
        "X = input_sequences[:, :-1]   ## Contains all the tokens in each sequence except for the last one, representing the input context\n",
        "y = input_sequences[:, -1]    ## The values of the last column in the ‘input_sequences’ array, which represents the target or predicted word\n",
        "\n",
        "## Convert output to one-hot encode vectors\n",
        "\n",
        "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))    ## Each target word is represented as a binary vector"
      ],
      "metadata": {
        "id": "WR3aAS63lDyx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Model Training - Neural Network Architecture\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQWMS9XEmAaF",
        "outputId": "be221101-a61f-4e2e-b49e-b484ef80a8f0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 20, 100)           899300    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 150)               150600    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 8993)              1357943   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,407,843\n",
            "Trainable params: 2,407,843\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=100, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r_s9JvNmcB3",
        "outputId": "0eb05098-bdc2-4e6f-a796-1e0a61e7419f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "3010/3010 [==============================] - 128s 42ms/step - loss: 6.2531 - accuracy: 0.0745\n",
            "Epoch 2/100\n",
            "3010/3010 [==============================] - 124s 41ms/step - loss: 5.5398 - accuracy: 0.1215\n",
            "Epoch 3/100\n",
            "3010/3010 [==============================] - 122s 40ms/step - loss: 5.1484 - accuracy: 0.1458\n",
            "Epoch 4/100\n",
            "3010/3010 [==============================] - 118s 39ms/step - loss: 4.8176 - accuracy: 0.1636\n",
            "Epoch 5/100\n",
            "3010/3010 [==============================] - 118s 39ms/step - loss: 4.5164 - accuracy: 0.1808\n",
            "Epoch 6/100\n",
            "3010/3010 [==============================] - 119s 40ms/step - loss: 4.2325 - accuracy: 0.2013\n",
            "Epoch 7/100\n",
            "3010/3010 [==============================] - 119s 39ms/step - loss: 3.9600 - accuracy: 0.2267\n",
            "Epoch 8/100\n",
            "3010/3010 [==============================] - 119s 40ms/step - loss: 3.7003 - accuracy: 0.2554\n",
            "Epoch 9/100\n",
            "3010/3010 [==============================] - 121s 40ms/step - loss: 3.4564 - accuracy: 0.2886\n",
            "Epoch 10/100\n",
            "3010/3010 [==============================] - 123s 41ms/step - loss: 3.2284 - accuracy: 0.3219\n",
            "Epoch 11/100\n",
            "3010/3010 [==============================] - 122s 41ms/step - loss: 3.0164 - accuracy: 0.3571\n",
            "Epoch 12/100\n",
            "3010/3010 [==============================] - 122s 40ms/step - loss: 2.8220 - accuracy: 0.3922\n",
            "Epoch 13/100\n",
            "3010/3010 [==============================] - 119s 39ms/step - loss: 2.6434 - accuracy: 0.4254\n",
            "Epoch 14/100\n",
            "3010/3010 [==============================] - 118s 39ms/step - loss: 2.4760 - accuracy: 0.4579\n",
            "Epoch 15/100\n",
            "3010/3010 [==============================] - 118s 39ms/step - loss: 2.3264 - accuracy: 0.4879\n",
            "Epoch 16/100\n",
            "3010/3010 [==============================] - 118s 39ms/step - loss: 2.1864 - accuracy: 0.5161\n",
            "Epoch 17/100\n",
            "3010/3010 [==============================] - 118s 39ms/step - loss: 2.0598 - accuracy: 0.5406\n",
            "Epoch 18/100\n",
            "3010/3010 [==============================] - 118s 39ms/step - loss: 1.9428 - accuracy: 0.5679\n",
            "Epoch 19/100\n",
            "3010/3010 [==============================] - 118s 39ms/step - loss: 1.8364 - accuracy: 0.5889\n",
            "Epoch 20/100\n",
            "3010/3010 [==============================] - 117s 39ms/step - loss: 1.7367 - accuracy: 0.6107\n",
            "Epoch 21/100\n",
            "3010/3010 [==============================] - 118s 39ms/step - loss: 1.6475 - accuracy: 0.6296\n",
            "Epoch 22/100\n",
            "3010/3010 [==============================] - 118s 39ms/step - loss: 1.5643 - accuracy: 0.6484\n",
            "Epoch 23/100\n",
            "3010/3010 [==============================] - 117s 39ms/step - loss: 1.4872 - accuracy: 0.6654\n",
            "Epoch 24/100\n",
            "3010/3010 [==============================] - 119s 39ms/step - loss: 1.4195 - accuracy: 0.6790\n",
            "Epoch 25/100\n",
            "3010/3010 [==============================] - 118s 39ms/step - loss: 1.3535 - accuracy: 0.6942\n",
            "Epoch 26/100\n",
            "3010/3010 [==============================] - 118s 39ms/step - loss: 1.2956 - accuracy: 0.7066\n",
            "Epoch 27/100\n",
            "3010/3010 [==============================] - 119s 39ms/step - loss: 1.2402 - accuracy: 0.7179\n",
            "Epoch 28/100\n",
            "3010/3010 [==============================] - 119s 39ms/step - loss: 1.1886 - accuracy: 0.7316\n",
            "Epoch 29/100\n",
            "3010/3010 [==============================] - 119s 39ms/step - loss: 1.1433 - accuracy: 0.7396\n",
            "Epoch 30/100\n",
            "3010/3010 [==============================] - 119s 40ms/step - loss: 1.0977 - accuracy: 0.7507\n",
            "Epoch 31/100\n",
            "3010/3010 [==============================] - 119s 39ms/step - loss: 1.0597 - accuracy: 0.7593\n",
            "Epoch 32/100\n",
            "3010/3010 [==============================] - 119s 40ms/step - loss: 1.0227 - accuracy: 0.7657\n",
            "Epoch 33/100\n",
            "3010/3010 [==============================] - 118s 39ms/step - loss: 0.9879 - accuracy: 0.7741\n",
            "Epoch 34/100\n",
            "3010/3010 [==============================] - 118s 39ms/step - loss: 0.9562 - accuracy: 0.7810\n",
            "Epoch 35/100\n",
            "3010/3010 [==============================] - 119s 40ms/step - loss: 0.9260 - accuracy: 0.7864\n",
            "Epoch 36/100\n",
            "3010/3010 [==============================] - 118s 39ms/step - loss: 0.9001 - accuracy: 0.7915\n",
            "Epoch 37/100\n",
            "3010/3010 [==============================] - 118s 39ms/step - loss: 0.8746 - accuracy: 0.7984\n",
            "Epoch 38/100\n",
            "3010/3010 [==============================] - 117s 39ms/step - loss: 0.8532 - accuracy: 0.8027\n",
            "Epoch 39/100\n",
            "3010/3010 [==============================] - 119s 40ms/step - loss: 0.8332 - accuracy: 0.8076\n",
            "Epoch 40/100\n",
            "3010/3010 [==============================] - 117s 39ms/step - loss: 0.8091 - accuracy: 0.8130\n",
            "Epoch 41/100\n",
            "3010/3010 [==============================] - 119s 40ms/step - loss: 0.7929 - accuracy: 0.8147\n",
            "Epoch 42/100\n",
            "3010/3010 [==============================] - 119s 39ms/step - loss: 0.7754 - accuracy: 0.8185\n",
            "Epoch 43/100\n",
            "3010/3010 [==============================] - 119s 39ms/step - loss: 0.7581 - accuracy: 0.8225\n",
            "Epoch 44/100\n",
            "3010/3010 [==============================] - 121s 40ms/step - loss: 0.7424 - accuracy: 0.8259\n",
            "Epoch 45/100\n",
            "3010/3010 [==============================] - 120s 40ms/step - loss: 0.7297 - accuracy: 0.8285\n",
            "Epoch 46/100\n",
            "3010/3010 [==============================] - 120s 40ms/step - loss: 0.7170 - accuracy: 0.8308\n",
            "Epoch 47/100\n",
            "3010/3010 [==============================] - 121s 40ms/step - loss: 0.7071 - accuracy: 0.8324\n",
            "Epoch 48/100\n",
            "3010/3010 [==============================] - 121s 40ms/step - loss: 0.6943 - accuracy: 0.8350\n",
            "Epoch 49/100\n",
            "3010/3010 [==============================] - 122s 40ms/step - loss: 0.6823 - accuracy: 0.8369\n",
            "Epoch 50/100\n",
            "3010/3010 [==============================] - 119s 40ms/step - loss: 0.6726 - accuracy: 0.8404\n",
            "Epoch 51/100\n",
            "3010/3010 [==============================] - 119s 40ms/step - loss: 0.6641 - accuracy: 0.8408\n",
            "Epoch 52/100\n",
            "3010/3010 [==============================] - 120s 40ms/step - loss: 0.6557 - accuracy: 0.8425\n",
            "Epoch 53/100\n",
            "3010/3010 [==============================] - 120s 40ms/step - loss: 0.6471 - accuracy: 0.8452\n",
            "Epoch 54/100\n",
            "3010/3010 [==============================] - 120s 40ms/step - loss: 0.6398 - accuracy: 0.8449\n",
            "Epoch 55/100\n",
            "3010/3010 [==============================] - 120s 40ms/step - loss: 0.6325 - accuracy: 0.8477\n",
            "Epoch 56/100\n",
            "3010/3010 [==============================] - 120s 40ms/step - loss: 0.6256 - accuracy: 0.8486\n",
            "Epoch 57/100\n",
            "3010/3010 [==============================] - 120s 40ms/step - loss: 0.6219 - accuracy: 0.8484\n",
            "Epoch 58/100\n",
            "3010/3010 [==============================] - 121s 40ms/step - loss: 0.6135 - accuracy: 0.8502\n",
            "Epoch 59/100\n",
            "3010/3010 [==============================] - 122s 41ms/step - loss: 0.6093 - accuracy: 0.8510\n",
            "Epoch 60/100\n",
            "3010/3010 [==============================] - 122s 41ms/step - loss: 0.5996 - accuracy: 0.8534\n",
            "Epoch 61/100\n",
            "3010/3010 [==============================] - 123s 41ms/step - loss: 0.6002 - accuracy: 0.8527\n",
            "Epoch 62/100\n",
            "3010/3010 [==============================] - 121s 40ms/step - loss: 0.5949 - accuracy: 0.8534\n",
            "Epoch 63/100\n",
            "3010/3010 [==============================] - 120s 40ms/step - loss: 0.5914 - accuracy: 0.8545\n",
            "Epoch 64/100\n",
            "3010/3010 [==============================] - 120s 40ms/step - loss: 0.5885 - accuracy: 0.8539\n",
            "Epoch 65/100\n",
            "3010/3010 [==============================] - 122s 41ms/step - loss: 0.5823 - accuracy: 0.8546\n",
            "Epoch 66/100\n",
            "3010/3010 [==============================] - 121s 40ms/step - loss: 0.5784 - accuracy: 0.8565\n",
            "Epoch 67/100\n",
            "3010/3010 [==============================] - 121s 40ms/step - loss: 0.5714 - accuracy: 0.8577\n",
            "Epoch 68/100\n",
            "3010/3010 [==============================] - 121s 40ms/step - loss: 0.5722 - accuracy: 0.8574\n",
            "Epoch 69/100\n",
            "3010/3010 [==============================] - 123s 41ms/step - loss: 0.5676 - accuracy: 0.8577\n",
            "Epoch 70/100\n",
            "3010/3010 [==============================] - 120s 40ms/step - loss: 0.5644 - accuracy: 0.8587\n",
            "Epoch 71/100\n",
            "3010/3010 [==============================] - 124s 41ms/step - loss: 0.5610 - accuracy: 0.8596\n",
            "Epoch 72/100\n",
            "3010/3010 [==============================] - 123s 41ms/step - loss: 0.5632 - accuracy: 0.8583\n",
            "Epoch 73/100\n",
            "3010/3010 [==============================] - 121s 40ms/step - loss: 0.5581 - accuracy: 0.8596\n",
            "Epoch 74/100\n",
            "3010/3010 [==============================] - 124s 41ms/step - loss: 0.5542 - accuracy: 0.8603\n",
            "Epoch 75/100\n",
            "3010/3010 [==============================] - 121s 40ms/step - loss: 0.5561 - accuracy: 0.8582\n",
            "Epoch 76/100\n",
            "3010/3010 [==============================] - 124s 41ms/step - loss: 0.5540 - accuracy: 0.8593\n",
            "Epoch 77/100\n",
            "3010/3010 [==============================] - 122s 40ms/step - loss: 0.5487 - accuracy: 0.8606\n",
            "Epoch 78/100\n",
            "3010/3010 [==============================] - 120s 40ms/step - loss: 0.5467 - accuracy: 0.8611\n",
            "Epoch 79/100\n",
            "3010/3010 [==============================] - 119s 40ms/step - loss: 0.5501 - accuracy: 0.8586\n",
            "Epoch 80/100\n",
            "3010/3010 [==============================] - 117s 39ms/step - loss: 0.5452 - accuracy: 0.8610\n",
            "Epoch 81/100\n",
            "3010/3010 [==============================] - 118s 39ms/step - loss: 0.5417 - accuracy: 0.8612\n",
            "Epoch 82/100\n",
            "3010/3010 [==============================] - 119s 40ms/step - loss: 0.5361 - accuracy: 0.8643\n",
            "Epoch 83/100\n",
            "3010/3010 [==============================] - 118s 39ms/step - loss: 0.5386 - accuracy: 0.8615\n",
            "Epoch 84/100\n",
            "3010/3010 [==============================] - 120s 40ms/step - loss: 0.5344 - accuracy: 0.8628\n",
            "Epoch 85/100\n",
            "3010/3010 [==============================] - 117s 39ms/step - loss: 0.5354 - accuracy: 0.8639\n",
            "Epoch 86/100\n",
            "3010/3010 [==============================] - 116s 39ms/step - loss: 0.5316 - accuracy: 0.8638\n",
            "Epoch 87/100\n",
            "3010/3010 [==============================] - 117s 39ms/step - loss: 0.5387 - accuracy: 0.8608\n",
            "Epoch 88/100\n",
            "3010/3010 [==============================] - 115s 38ms/step - loss: 0.5337 - accuracy: 0.8620\n",
            "Epoch 89/100\n",
            "3010/3010 [==============================] - 117s 39ms/step - loss: 0.5300 - accuracy: 0.8633\n",
            "Epoch 90/100\n",
            "3010/3010 [==============================] - 117s 39ms/step - loss: 0.5294 - accuracy: 0.8641\n",
            "Epoch 91/100\n",
            "3010/3010 [==============================] - 117s 39ms/step - loss: 0.5312 - accuracy: 0.8621\n",
            "Epoch 92/100\n",
            "3010/3010 [==============================] - 118s 39ms/step - loss: 0.5278 - accuracy: 0.8643\n",
            "Epoch 93/100\n",
            "3010/3010 [==============================] - 118s 39ms/step - loss: 0.5315 - accuracy: 0.8617\n",
            "Epoch 94/100\n",
            "3010/3010 [==============================] - 115s 38ms/step - loss: 0.5275 - accuracy: 0.8622\n",
            "Epoch 95/100\n",
            "3010/3010 [==============================] - 116s 38ms/step - loss: 0.5256 - accuracy: 0.8629\n",
            "Epoch 96/100\n",
            "3010/3010 [==============================] - 115s 38ms/step - loss: 0.5259 - accuracy: 0.8629\n",
            "Epoch 97/100\n",
            "3010/3010 [==============================] - 116s 39ms/step - loss: 0.5261 - accuracy: 0.8627\n",
            "Epoch 98/100\n",
            "3010/3010 [==============================] - 118s 39ms/step - loss: 0.5259 - accuracy: 0.8622\n",
            "Epoch 99/100\n",
            "3010/3010 [==============================] - 128s 43ms/step - loss: 0.5185 - accuracy: 0.8652\n",
            "Epoch 100/100\n",
            "3010/3010 [==============================] - 129s 43ms/step - loss: 0.5152 - accuracy: 0.8658\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7d42020d79a0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"Can we talk instead of\"\n",
        "next_words = 4\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C42cWn8nbnur",
        "outputId": "c83daa97-7c17-4a0e-f502-8567f3423cd9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "Can we talk instead of reportedly biobehavioral family afford\n"
          ]
        }
      ]
    }
  ]
}